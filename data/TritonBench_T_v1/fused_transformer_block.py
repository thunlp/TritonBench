import torch
import torch.nn.functional as F
from typing import Optional
def fused_transformer_block(
        input: torch.Tensor, 
        weight1: torch.Tensor, 
        weight2: torch.Tensor, 
        residual: torch.Tensor, 
        dropout_p: float=0.1, 
        eps: float=1e-05, 
        *, out: Optional[torch.Tensor]=None) -> torch.Tensor:
    """
    Performs a sequence of operations commonly used in transformer models.

    Arguments:
    - input (Tensor): Input tensor of shape (*, N, D_in), where * denotes any number of batch dimensions.
    - weight1 (Tensor): Weight matrix of shape (D_in, D_k).
    - weight2 (Tensor): Weight matrix of shape (D_k, D_out).
    - residual (Tensor): Residual tensor to be added before layer normalization, must be broadcastable to the shape of Z_4.
    - dropout_p (float, optional): Probability of an element to be zeroed in the dropout layer. Default: 0.1.
    - eps (float, optional): A value added to the denominator for numerical stability in layer normalization. Default: 1e-5.
    - out (Tensor, optional): Output tensor. Ignored if None. Default: None.

    Returns:
    - Tensor: The output tensor after performing the sequence of operations.
    """
    z1 = input @ weight1
    z2 = F.softmax(z1, dim=-1)
    z3 = F.dropout(z2, p=dropout_p, training=True)
    z4 = z3 @ weight2
    y = F.layer_norm(z4 + residual, normalized_shape=(z4.size(-1),), eps=eps)
    if out is not None:
        out.copy_(y)
        return out
    return y

##################################################################################################################################################


import torch
torch.manual_seed(42)

def test_fused_transformer_block():
    results = {}

    # Test case 1: Basic functionality test
    input1 = torch.randn(2, 3, 4, device='cuda')
    weight1_1 = torch.randn(4, 5, device='cuda')
    weight2_1 = torch.randn(5, 4, device='cuda')
    residual1 = torch.randn(2, 3, 4, device='cuda')
    results["test_case_1"] = fused_transformer_block(input1, weight1_1, weight2_1, residual1)

    # Test case 2: Different input size
    input2 = torch.randn(1, 5, 6, device='cuda')
    weight1_2 = torch.randn(6, 7, device='cuda')
    weight2_2 = torch.randn(7, 6, device='cuda')
    residual2 = torch.randn(1, 5, 6, device='cuda')
    results["test_case_2"] = fused_transformer_block(input2, weight1_2, weight2_2, residual2)

    # Test case 3: Test with dropout probability set to 0
    input3 = torch.randn(3, 2, 4, device='cuda')
    weight1_3 = torch.randn(4, 5, device='cuda')
    weight2_3 = torch.randn(5, 4, device='cuda')
    residual3 = torch.randn(3, 2, 4, device='cuda')
    results["test_case_3"] = fused_transformer_block(input3, weight1_3, weight2_3, residual3, dropout_p=0.0)

    # Test case 4: Test with a different epsilon value
    input4 = torch.randn(4, 3, 5, device='cuda')
    weight1_4 = torch.randn(5, 6, device='cuda')
    weight2_4 = torch.randn(6, 5, device='cuda')
    residual4 = torch.randn(4, 3, 5, device='cuda')
    results["test_case_4"] = fused_transformer_block(input4, weight1_4, weight2_4, residual4, eps=1e-3)

    return results

test_results = test_fused_transformer_block()
print(test_results)